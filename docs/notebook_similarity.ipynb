{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook similarity job demo\n",
    "\n",
    "This shows how you might replace the Rails `notebook_summaries` job with Python using this library.\n",
    "\n",
    "Note that we use our ORM interface to retrieve the notebooks, but then use SQLAlchemy's core interface to update the `notebook_summaries` table in the database.  SQLAlchemy ORM does support bulk operations, but using that interface is [discouraged](https://docs.sqlalchemy.org/en/14/orm/persistence_techniques.html#bulk-operations) in favor of the core interface -- plus we don't have any need to manipulate NotebookSummary models as objects anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sqlalchemy as sa\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nbgallery.database as nbgdb\n",
    "import nbgallery.database.orm as nbgorm\n",
    "import nbgallery.notebooks as nbgnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF computation\n",
    "\n",
    "We'll use the [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from scikit-learn to generate a notebook-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = nbgorm.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "index = {} # matrix row/column id => notebook id\n",
    "for i, nb in enumerate(session.query(nbgorm.Notebook).all()):\n",
    "    index[i] = nb.id\n",
    "    doc = nbgnb.from_model(nb)\n",
    "    corpus.append(' '.join(doc.sources()))\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're including code, so to avoid getting numeric contants, let's change\n",
    "# the default token pattern to require words start with a letter.\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'(?u)\\b[a-z]\\w+\\b')\n",
    "tfidf = vectorizer.fit_transform(corpus)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job parameters\n",
    "\n",
    "For each notebook, we'll keep the 5 most similar notebooks as long as the similarity score is > 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_top_n = 5\n",
    "min_score = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big memory version\n",
    "\n",
    "We'll use scikit-learn's [cosine similarity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) to compute a notebook-notebook similarity matrix from the TF-IDF notebook-term matrix.\n",
    "\n",
    "In this \"big memory\" version, we get a dense matrix back and then add all the database entries in one big bulk insert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_similarity = cosine_similarity(tfidf)\n",
    "doc_similarity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll convert to a dataframe just for some variety; we could leave it as a matrix and access directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(doc_similarity)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through and find the top n most similar notebooks for each notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "entries = [] # rows to be added to the database\n",
    "\n",
    "for i, column in df.items():\n",
    "    nbid_i = index[i]\n",
    "    #print(f\"column {i}, notebook {nbid_i}\")\n",
    "    top_n = []\n",
    "    for j, score in column.sort_values(ascending=False, inplace=False).items():\n",
    "        if len(top_n) == keep_top_n or score < min_score:\n",
    "            break\n",
    "        if i == j:\n",
    "            continue\n",
    "        nbid_j = index[j]\n",
    "        top_n.append((nbid_j, score))\n",
    "        entries.append({\n",
    "            'notebook_id': nbid_i,\n",
    "            'other_notebook_id': nbid_j,\n",
    "            'score': score,\n",
    "            'created_at': now, \n",
    "            'updated_at': now\n",
    "        })\n",
    "    #print(' ', top_n)\n",
    "\n",
    "print(len(entries))\n",
    "print(entries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear out all the old entries in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = nbgorm.NotebookSimilarity.__table__\n",
    "nbgdb.engine.execute(table.delete())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert all the new entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert = table.insert().values(entries)\n",
    "#print(insert.compile(dialect=sa.dialects.mysql.dialect()))\n",
    "nbgdb.engine.execute(insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(table.select(), nbgdb.engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small memory version\n",
    "\n",
    "In this \"small memory\" version, we'll ask for the cosine similarity as a sparse matrix.  Then we'll delete and insert once per notebook, instead of one giant delete and insert for the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_similarity = cosine_similarity(tfidf, dense_output=False)\n",
    "sparse_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = nbgorm.NotebookSimilarity.__table__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, lop through and find the top n most similar notebooks for each notebook, but update the database notebook by notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(sparse_similarity):\n",
    "    nbid_i = index[i]\n",
    "    #print(f\"column {i}, notebook {nbid_i}\")\n",
    "    now = datetime.datetime.now()\n",
    "    entries = []\n",
    "    top_n = []\n",
    "    coo = row.tocoo()\n",
    "    for j, score in sorted(zip(coo.col, coo.data), key=lambda z : z[1], reverse=True):\n",
    "        if len(top_n) == keep_top_n or score < min_score:\n",
    "            break\n",
    "        if i == j:\n",
    "            continue\n",
    "        nbid_j = index[j]\n",
    "        top_n.append((nbid_j, score))\n",
    "        entries.append({\n",
    "            'notebook_id': nbid_i,\n",
    "            'other_notebook_id': nbid_j,\n",
    "            'score': score,\n",
    "            'created_at': now, \n",
    "            'updated_at': now\n",
    "        })\n",
    "    #print(' ', top_n)\n",
    "    delete = table.delete().where(table.c.notebook_id == nbid_i)\n",
    "    nbgdb.engine.execute(delete)\n",
    "    insert = table.insert().values(entries)\n",
    "    nbgdb.engine.execute(insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(table.select(), nbgdb.engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
